# E-commerce-Sales-Analytics-Data-Pipeline-Using-Spark-Airflow-and-Docker
The goal is to set up a data pipeline using Apache Airflow for workflow automation, Apache Spark for processing large datasets, and Docker for scalability. This project teaches data engineering basics with real-world tools.

## Key Components

1. **Data Ingestion**: Use a small CSV file with sales data.
2. **Data Processing**: Use Spark for basic transformations and aggregations.
3. **Orchestration**: Use Airflow to automate and schedule the Spark processing.
4. **Containerization**: Use Docker to containerize Airflow and Spark.
