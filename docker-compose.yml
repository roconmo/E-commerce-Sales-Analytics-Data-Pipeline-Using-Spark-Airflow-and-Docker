services:
  #airflow-init:
  #  build: ./airflow
  #  command: airflow db init
  #  volumes:
  #    - ./airflow:/opt/airflow  # Map local directory to container
  #  entrypoint: /bin/bash -c "airflow db init"
  airflow-init:
    build: ./airflow
    command: airflow db init #"db init"
    volumes:
      - ./airflow:/opt/airflow  # Map local directory to container

  airflow-webserver:
    build: ./airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    ports:
      - "8081:8080"
    volumes:
      - ./airflow:/opt/airflow  # Ensure the same volume is shared
    depends_on:
      - airflow-init
    command: bash -c "airflow db upgrade && airflow users create --username admin --firstname Rosalia --lastname Contreras --role Admin --email rosalia.contreras@optimissa.com --password admin && airflow webserver"

  airflow-scheduler:
    build: ./airflow
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow:/opt/airflow  # Ensure the same volume is shared
    depends_on:
      - airflow-init
    command: airflow scheduler

  spark:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
    ports:
      - "8082:8080"  # Spark UI accessible on localhost:8082
    volumes:
      - ./data:/data
